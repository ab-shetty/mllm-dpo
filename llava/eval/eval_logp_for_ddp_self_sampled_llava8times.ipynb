{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload the interleaf data \n",
    "import pandas as pd\n",
    "df = pd.read_csv('/home/ubuntu/latest_llava/LLaVA/playground/data/dpo/1231-13b-selfsamples/lrv_1699__self-sampled_at_least2_in_diff_for_dpo_inference1230.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['image', 'question'])\n",
    "filtered_df = grouped.filter(lambda x: len(x) == 4)\n",
    "len(filtered_df), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Output'] = df['cur_candidate_answer']\n",
    "del df['cur_candidate_answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following turns it into SteerLM format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your original DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Group the DataFrame by 'image'\n",
    "for image, group in df.groupby(['image', 'question']):\n",
    "    # Find the 'GPT4V_response' and candidate responses within this group\n",
    "    candidate_responses = group\n",
    "    # If there are no candidate responses, skip this group\n",
    "    if candidate_responses.empty:\n",
    "        continue\n",
    "    \n",
    "    # Calculate the difference in the total sum of 'Helpfulness', 'Correctness', and 'Coherence'\n",
    "    candidate_responses['Sum_quality'] = candidate_responses['Helpfulness'] + candidate_responses['Correctness'] + candidate_responses['Coherence']\n",
    "    diff = abs(max(candidate_responses['Sum_quality']) - min(candidate_responses['Sum_quality']))\n",
    "    \n",
    "    # If the difference is too small, skip this group\n",
    "    if diff < 2:  # Define your threshold\n",
    "        continue\n",
    "    \n",
    "    # Find the response with the highest 'Sum_quality'\n",
    "    max_row_index = candidate_responses['Sum_quality'].idxmax()\n",
    "    max_row = candidate_responses.loc[max_row_index]\n",
    "    \n",
    "    # Filter for responses that are at least 2 lesser or equal\n",
    "    filtered_responses = candidate_responses[candidate_responses['Sum_quality'] <= max_row['Sum_quality'] - 2]\n",
    "    \n",
    "    # If the filtered group is empty, skip this group\n",
    "    if filtered_responses.empty:\n",
    "        continue\n",
    "    \n",
    "    # Randomly sample one response from the filtered group\n",
    "    selected_row = filtered_responses.sample(n=1)\n",
    "    assert max_row['prompt'] == selected_row['prompt'].item()\n",
    "    # Create a new row with the 'GPT4V_response' and the selected candidate response\n",
    "    new_row = {\n",
    "        'image': max_row['image'],\n",
    "        'question': max_row['prompt'],\n",
    "        # 'GPT4V_question_and_response': max_row['conversations'],  # Add 'conversations' column\n",
    "        'chosen': max_row['Output'],\n",
    "        # 'Candidate_question_and_response': selected_row['conversations'],  # Add 'conversations' column\n",
    "        'rejected': selected_row['Output'].item(),\n",
    "        'Helpfulness_gpt4v': max_row['Helpfulness'],\n",
    "        'Correctness_gpt4v': max_row['Correctness'],\n",
    "        'Coherence_gpt4v': max_row['Coherence'],\n",
    "        'Sum_of_helpfulness_correctness_coherence_gpt4v': max_row['Helpfulness'] + max_row['Correctness'] + max_row['Coherence'],\n",
    "        'Complexity_gpt4v': max_row['Complexity'],\n",
    "        'Verbosity_gpt4v': max_row['Verbosity'],\n",
    "        'Helpfulness_candidate': selected_row['Helpfulness'].values[0],\n",
    "        'Correctness_candidate': selected_row['Correctness'].values[0],\n",
    "        'Coherence_candidate': selected_row['Coherence'].values[0],\n",
    "        'Sum_of_helpfulness_correctness_coherence_candidate': selected_row['Helpfulness'].values[0] + selected_row['Correctness'].values[0] + selected_row['Coherence'].values[0],\n",
    "        'Complexity_candidate': selected_row['Complexity'].values[0],\n",
    "        'Verbosity_candidate': selected_row['Verbosity'].values[0],\n",
    "    }\n",
    "    \n",
    "    new_rows.append(new_row)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "sum_attributes = ['Sum_of_helpfulness_correctness_coherence_gpt4v', \n",
    " 'Sum_of_helpfulness_correctness_coherence_candidate']\n",
    "new_df[sum_attributes].describe()\n",
    "\n",
    "def form_multi_turn(row):\n",
    "    first_turn = {'from': 'human', 'value': row['question']}\n",
    "    \n",
    "    first_candidate = {'from': 'gpt', 'value': row['chosen']}\n",
    "    candidate_attribute_str = f'''The previous answer is in the following quality metrics in the 0-4 likert scale: Helpfulness: {row['Helpfulness_candidate']}, Correctness: {row['Correctness_candidate']}, Coherence: {row['Coherence_candidate']}, Complexity: {row['Complexity_candidate']}, Verbosity: {row['Verbosity_candidate']}.\n",
    "    Please answer the question such that your response is in the following attributes in 5-point Likert scale: Helpfulness: {row['Helpfulness_gpt4v']}, Correctness: {row['Correctness_gpt4v']}, Coherence: {row['Coherence_gpt4v']}, Complexity: {row['Complexity_gpt4v']}, Verbosity: {row['Verbosity_gpt4v']}. '''\n",
    "    preference = {'from': 'human', 'value': candidate_attribute_str}\n",
    "    final_turn = {'from': 'gpt', 'value': row['rejected']}\n",
    "    return [first_turn, first_candidate, preference, final_turn]\n",
    "\n",
    "turn_into_steerlm = False\n",
    "if turn_into_steerlm:\n",
    "    new_df['conversations'] = new_df.apply(form_multi_turn, axis=1)\n",
    "    new_df['image'] = 'scigraph/' + new_df['image'].astype(str)\n",
    "    new_df.to_json('/home/ubuntu/latest_llava/LLaVA/playground/data/concatnateed_helpsteer/scigraph3000_self-sampling.json', orient='records')\n",
    "# new_df['conversations'] = new_df.apply(form_multi_turn, axis=1)\n",
    "# new_df['image'] = 'scigraph/' + new_df['image'].astype(str)\n",
    "# new_df.to_json('/home/ubuntu/latest_llava/LLaVA/playground/data/concatnateed_helpsteer/scigraph3000_self-sampling.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['chosen'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_csv = '/home/ubuntu/latest_llava/LLaVA/playground/data/dpo/lrv-10k-13k-sampled_at_least2_in_diff_for_dpo_inference0104.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The start of a json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "intermediate_csv = '/home/ubuntu/latest_llava/LLaVA/playground/data/dpo/lrv-10k-13k-sampled_at_least2_in_diff_for_dpo_inference0104.csv'\n",
    "\n",
    "df = pd.read_csv(intermediate_csv)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "from PIL import Image\n",
    "import math\n",
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "from datetime import date\n",
    "import torch.utils.data as torch_data\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/latest_llava/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "# model_base = '/home/ubuntu/latest_llava/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)\n",
    "\n",
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER=\"/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017/\"\n",
    "intermediate_csv = '/home/ubuntu/latest_llava/LLaVA/playground/data/dpo/lrv-10k-13k-sampled_at_least2_in_diff_for_dpo_inference0104.csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(intermediate_csv)\n",
    "intermediate_json = intermediate_csv.split('csv')[0] + 'json'\n",
    "\n",
    "\n",
    "df.to_json(intermediate_json, orient='records')\n",
    "\n",
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n",
    "def _convert_to_llava_question_turn(question):\n",
    "    return {\"from\": \"human\", \"value\": \"<image>\\n\" + question}\n",
    "\n",
    "# def _convert_to_llava_question_turn(question):\n",
    "#     return {\"from\": \"human\", \"value\": question}\n",
    "\n",
    "df['question'] = df['question'].apply(_convert_to_llava_question_turn)\n",
    "df['chosen'] = df['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "df['rejected'] = df['rejected'].apply(_convert_to_llava_answer_turn)\n",
    "\n",
    "df.to_json(intermediate_json, orient='records')\n",
    "\n",
    "# FILEPATH: /home/ubuntu/latest_llava/LLaVA/llava/eval/eval_logp_for_ddp_self_sampled_llava8times.ipynb\n",
    "output_path = intermediate_csv.split('csv')[0] + f'with_logp{model_name}_{date.today()}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import DPODataset\n",
    "multimodal_cfg = {\n",
    "    'image_processor': image_processor,\n",
    "    'is_multimodal': True,\n",
    "    # 'image_token_len': image_token_len, # TODO check if needed\n",
    "    'use_im_start_end': use_im_start_end,\n",
    "    'image_aspect_ratio': 'pad', \n",
    "    'data_path': intermediate_json,\n",
    "    'image_folder': IMAGE_FOLDER,\n",
    "}\n",
    "dpo_dataset = DPODataset(tokenizer, multimodal_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset[0][0]['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dpo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import DataCollatorForDPODataset\n",
    "\n",
    "data_collator = DataCollatorForDPODataset(tokenizer, beta=0.1, mod_token_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch_data.DataLoader(dpo_dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logps = get_batch_logps(model, dataloader, data_collator, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits_cleaned = output_logits[:, 575:]\n",
    "output_logits_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dpo_dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits[:, 575:], labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(intermediate_json)\n",
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "# Add each list as a column to the dataframe with its variable name\n",
    "df['win_logp'] = win_logp_list\n",
    "df['rej_logp'] = rej_logp_list\n",
    "df['win_avg_logp'] = win_avg_logp_list\n",
    "df['rej_avg_logp'] = rej_avg_logp_list\n",
    "df['win_per_token_logp'] = win_per_token_logp_list\n",
    "df['rej_per_token_logp'] = rej_per_token_logp_list\n",
    "exisitng_columns = ['rej_logp', \n",
    "                    'win_logp',\n",
    "                    'rej_avg_logp',\n",
    "                    'win_avg_logp',\n",
    "                    'rej_per_token_logp',\n",
    "                    'win_per_token_logp']\n",
    "for col in exisitng_columns:\n",
    "    rename = 'ref_' + col\n",
    "    df[rename] = df[col]\n",
    "    del df[col]\n",
    "# df['chosen'] = df['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "# df['rejected'] = df['rejected'].apply(_convert_to_llava_answer_turn)\n",
    "# def _convert_to_llava_question_turn(question):\n",
    "#     return {\"from\": \"human\", \"value\": question}\n",
    "# df['question'] = df['question'].apply(_convert_to_llava_question_turn)\n",
    "df.to_json(output_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import shortuuid\n",
    "\n",
    "# from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "# from llava.conversation import conv_templates, SeparatorStyle\n",
    "# from llava.model.builder import load_pretrained_model\n",
    "# from llava.utils import disable_torch_init\n",
    "# from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "# from llava.train.train import preference_collator_fn\n",
    "# from functools import partial\n",
    "# import tqdm\n",
    "# from llava.train.llava_trainer import get_batch_logps\n",
    "# from PIL import Image\n",
    "# import math\n",
    "# from llava.train.train import encode_multimodal_preference_sample\n",
    "# from datetime import date\n",
    "# import torch.utils.data as torch_data\n",
    "# disable_torch_init()\n",
    "# model_path = '/home/ubuntu/latest_llava/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "# # model_base = '/home/ubuntu/latest_llava/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "# model_name = get_model_name_from_path(model_path)\n",
    "# tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)\n",
    "\n",
    "# use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "# IMAGE_FOLDER=\"/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017/\"\n",
    "# inference_data_path = intermediate_csv\n",
    "# # FILEPATH: /home/ubuntu/latest_llava/LLaVA/llava/eval/eval_logp_for_ddp_self_sampled_llava8times.ipynb\n",
    "# output_path = intermediate_csv.split('csv')[0] + f'with_logp{model_name}_{date.today()}.json'\n",
    "\n",
    "# class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "#     def __init__(self,\n",
    "#                  data_path,\n",
    "#                  tokenizer,\n",
    "#                  img_processor,\n",
    "#                  use_im_start_end):\n",
    "#         self.data = pd.read_csv(data_path)\n",
    "\n",
    "#         self.mm_cfg = {\n",
    "#             'image_processor': img_processor,\n",
    "#             'is_multimodal': True,\n",
    "#             # 'image_token_len': image_token_len, # TODO check if needed\n",
    "#             'use_im_start_end': use_im_start_end,\n",
    "#             'image_aspect_ratio': 'pad'\n",
    "#         }\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # After encode multimodal preference sample, \n",
    "#         # the image would ahve the pixel values values\n",
    "#         sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "#         rej_data_dict, win_data_dict = encode_multimodal_preference_sample(sample, self.tokenizer, self.mm_cfg)\n",
    "#         return rej_data_dict, win_data_dict\n",
    "    \n",
    "#     def _get_image(self, img_filename):\n",
    "#         img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "#         image = Image.open(img_path).convert('RGB')\n",
    "#         return image\n",
    "    \n",
    "\n",
    "#     def _convert_to_llava_answer_turn(self, answer):\n",
    "#         return {\"from\": \"gpt\", \"value\": answer}\n",
    "    \n",
    "#     def _convert_to_llava_question_turn(self, question):\n",
    "#         return {\"from\": \"human\", \"value\": \"<image>\\n\" + question}\n",
    "\n",
    "#     def convert_dataframe_row_to_source(self, row):\n",
    "#         dict_output = {'question': self._convert_to_llava_question_turn(row['question']), \n",
    "#                        # Chosen and rejected based on the sum of the first three quality score\n",
    "#                     'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "#                     'rejected': self._convert_to_llava_answer_turn(row['rejected']),\n",
    "#                     'image': self._get_image(row['image'])}\n",
    "#         return dict_output\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset(inference_data_path,\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]\n",
    "\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')\n",
    "\n",
    "df = pd.read_csv(inference_data_path)\n",
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "# Add each list as a column to the dataframe with its variable name\n",
    "df['win_logp'] = win_logp_list\n",
    "df['rej_logp'] = rej_logp_list\n",
    "df['win_avg_logp'] = win_avg_logp_list\n",
    "df['rej_avg_logp'] = rej_avg_logp_list\n",
    "df['win_per_token_logp'] = win_per_token_logp_list\n",
    "df['rej_per_token_logp'] = rej_per_token_logp_list\n",
    "exisitng_columns = ['rej_logp', \n",
    "                    'win_logp',\n",
    "                    'rej_avg_logp',\n",
    "                    'win_avg_logp',\n",
    "                    'rej_per_token_logp',\n",
    "                    'win_per_token_logp']\n",
    "for col in exisitng_columns:\n",
    "    rename = 'ref_' + col\n",
    "    df[rename] = df[col]\n",
    "    del df[col]\n",
    "df['chosen'] = df['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "df['rejected'] = df['rejected'].apply(_convert_to_llava_answer_turn)\n",
    "def _convert_to_llava_question_turn(question):\n",
    "    return {\"from\": \"human\", \"value\": question}\n",
    "df['question'] = df['question'].apply(_convert_to_llava_question_turn)\n",
    "df.to_json(output_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "679-104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape, labels.shape, attention_mask.shape, batch['images'].half().cuda().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df[['ref_rej_logp', 'ref_win_logp']].boxplot()\n",
    "\n",
    "plt.title('13b-007dpo-checkpoint Log Probability on preference dataset before DPO', fontsize=10)\n",
    "\n",
    "# Set the x-ticks\n",
    "plt.xticks([1, 2], ['Rejected Responses', 'Preferred Reponses'])\n",
    "plt.ylabel('Log Probability', fontsize=12)\n",
    "\n",
    "# Save the plot in high DPI and PDF format\n",
    "plt.savefig('output.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reload['chosen'] = df_reload['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "df_reload['rejected'] = df_reload['rejected'].apply(_convert_to_llava_answer_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reload['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('/home/ubuntu/LLaVA/playground/data/dpo/dpo_logp.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old draft below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect previous setup\n",
    "\n",
    "df_muffin_tsv = '/home/ubuntu/muffin/data/RLHF-V-Hall_v0/RLHF-V-Hall_v0-1837.tsv'\n",
    "df = pd.read_csv(df_muffin_tsv, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 tokenizer,\n",
    "                 img_processor,\n",
    "                 use_im_start_end):\n",
    "        self.data = pd.read_csv(data_path, sep='\\t')\n",
    "\n",
    "        self.mm_cfg = {\n",
    "            'image_processor': img_processor,\n",
    "            'is_multimodal': True,\n",
    "            # 'image_token_len': image_token_len, # TODO check if needed\n",
    "            'use_im_start_end': use_im_start_end,\n",
    "            'image_aspect_ratio': 'pad'\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # After encode multimodal preference sample, \n",
    "        # the image would ahve the pixel values values\n",
    "        sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "        rej_data_dict, win_data_dict = encode_multimodal_preference_sample(sample, self.tokenizer, self.mm_cfg)\n",
    "        return rej_data_dict, win_data_dict\n",
    "    \n",
    "    def _get_image(self, img_filename):\n",
    "        img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def _convert_to_llava_answer_turn(self, answer):\n",
    "        return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n",
    "    def convert_dataframe_row_to_source(self, row):\n",
    "        dict_output = {'question': row['question'], \n",
    "                       # Chosen and rejected based on the sum of the first three quality score\n",
    "                    'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "                    'rejected': self._convert_to_llava_answer_turn(row['rejected']),\n",
    "                    'image': self._get_image(row['image'])}\n",
    "        return dict_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset('/home/ubuntu/muffin/data/RLHF-V-Hall_v0/RLHF-V-Hall_v0-1837.tsv',\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_torch_dataset.mm_cfg['image_aspect_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_llava_answer_turn(self, answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_torch_dataset[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
