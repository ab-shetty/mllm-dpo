{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload the interleaf data \n",
    "import pandas as pd\n",
    "df = pd.read_json('/home/ubuntu/latest_llava/LLaVA/playground/data/steer_sft_model/training/interleaf_helpsteer_on_llava_lrv/extracted_attributes_sft_helpsteer_lrv3067llava3067-likert-1222_interleaved.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ouput is same as convesations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20632, 24536)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('image')\n",
    "filtered_df = grouped.filter(lambda x: len(x) == 4)\n",
    "len(filtered_df), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sum_of_helpfulness_correctness_coherence_gpt4v</th>\n",
       "      <th>Sum_of_helpfulness_correctness_coherence_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1888.000000</td>\n",
       "      <td>1888.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.121292</td>\n",
       "      <td>4.48411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.251211</td>\n",
       "      <td>2.41084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sum_of_helpfulness_correctness_coherence_gpt4v  \\\n",
       "count                                     1888.000000   \n",
       "mean                                         9.121292   \n",
       "std                                          2.251211   \n",
       "min                                          3.000000   \n",
       "25%                                          7.000000   \n",
       "50%                                          9.000000   \n",
       "75%                                         11.000000   \n",
       "max                                         12.000000   \n",
       "\n",
       "       Sum_of_helpfulness_correctness_coherence_candidate  \n",
       "count                                         1888.00000   \n",
       "mean                                             4.48411   \n",
       "std                                              2.41084   \n",
       "min                                              0.00000   \n",
       "25%                                              3.00000   \n",
       "50%                                              4.00000   \n",
       "75%                                              6.00000   \n",
       "max                                              9.00000   "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df is your original DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Group the DataFrame by 'image'\n",
    "for image, group in df.groupby('image'):\n",
    "    # Calculate the sum of 'Helpfulness', 'Correctness', and 'Coherence' for each candidate response\n",
    "    group['sum'] = group['Helpfulness'] + group['Correctness'] + group['Coherence']\n",
    "    \n",
    "    group = group[group['Source'].isin(['CandidateResponseA', 'CandidateResponseB'])].copy()\n",
    "    # Select the candidate response with the highest sum\n",
    "    highest_sum_response = group.loc[group['sum'].idxmax()]\n",
    "    \n",
    "    # Filter for the rest of the responses that are at least 2 points smaller in the sum\n",
    "    filtered_responses = group.loc[group['sum'] < highest_sum_response['sum'] - 2]\n",
    "    \n",
    "    # If the filtered candidate response is empty, continue\n",
    "    if filtered_responses.empty:\n",
    "        continue\n",
    "    \n",
    "    # Randomly sample 1 from the filtered responses\n",
    "    selected_response = filtered_responses.sample(1).iloc[0]\n",
    "    \n",
    "    # Create a new row with the selected candidate response and the higher quality one\n",
    "    new_row = {\n",
    "        'image': image,\n",
    "        'GPT4V_question_and_response': highest_sum_response['conversations'],  # Add 'conversations' column\n",
    "        'GPT4V_response': highest_sum_response['Output'],\n",
    "        'Candidate_question_and_response': selected_response['conversations'],  # Add 'conversations' column\n",
    "        'Candidate_response': selected_response['Output'],\n",
    "        'Helpfulness_gpt4v': highest_sum_response['Helpfulness'],\n",
    "        'Correctness_gpt4v': highest_sum_response['Correctness'],\n",
    "        'Coherence_gpt4v': highest_sum_response['Coherence'],\n",
    "        'Sum_of_helpfulness_correctness_coherence_gpt4v': highest_sum_response['sum'],\n",
    "        'Complexity_gpt4v': highest_sum_response['Complexity'],\n",
    "        'Verbosity_gpt4v': highest_sum_response['Verbosity'],\n",
    "        'Helpfulness_candidate': selected_response['Helpfulness'],\n",
    "        'Correctness_candidate': selected_response['Correctness'],\n",
    "        'Coherence_candidate': selected_response['Coherence'],\n",
    "        'Sum_of_helpfulness_correctness_coherence_candidate': selected_response['sum'],\n",
    "        'Complexity_candidate': selected_response['Complexity'],\n",
    "        'Verbosity_candidate': selected_response['Verbosity'],\n",
    "    }\n",
    "    \n",
    "    new_rows.append(new_row)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "sum_attributes = ['Sum_of_helpfulness_correctness_coherence_gpt4v', \n",
    " 'Sum_of_helpfulness_correctness_coherence_candidate']\n",
    "new_df[sum_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sum_of_helpfulness_correctness_coherence_gpt4v</th>\n",
       "      <th>Sum_of_helpfulness_correctness_coherence_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4587.000000</td>\n",
       "      <td>4587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.926313</td>\n",
       "      <td>6.538914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.446211</td>\n",
       "      <td>2.615321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sum_of_helpfulness_correctness_coherence_gpt4v  \\\n",
       "count                                     4587.000000   \n",
       "mean                                        11.926313   \n",
       "std                                          0.446211   \n",
       "min                                          0.000000   \n",
       "25%                                         12.000000   \n",
       "50%                                         12.000000   \n",
       "75%                                         12.000000   \n",
       "max                                         12.000000   \n",
       "\n",
       "       Sum_of_helpfulness_correctness_coherence_candidate  \n",
       "count                                        4587.000000   \n",
       "mean                                            6.538914   \n",
       "std                                             2.615321   \n",
       "min                                             0.000000   \n",
       "25%                                             4.000000   \n",
       "50%                                             7.000000   \n",
       "75%                                             9.000000   \n",
       "max                                            12.000000   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### GPT-4 vs Candidate Response ####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your original DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Group the DataFrame by 'image'\n",
    "for image, group in df.groupby('image'):\n",
    "    # Find the 'GPT4V_response' and candidate responses within this group\n",
    "    gpt4v_response = group[group['Source'] == 'GPT4V_response'].iloc[0]\n",
    "    candidate_responses = group[group['Source'].isin(['CandidateResponseA', 'CandidateResponseB'])]\n",
    "    \n",
    "    # If there are no candidate responses, skip this group\n",
    "    if candidate_responses.empty:\n",
    "        continue\n",
    "    \n",
    "    # Randomly select one of the candidate responses\n",
    "    candidate_response = candidate_responses.sample(1).iloc[0]\n",
    "\n",
    "    # Calculate the difference in the total sum of 'Helpfulness', 'Correctness', and 'Coherence'\n",
    "    diff = abs((gpt4v_response['Helpfulness'] + gpt4v_response['Correctness'] + gpt4v_response['Coherence']) -\n",
    "               (candidate_response['Helpfulness'] + candidate_response['Correctness'] + candidate_response['Coherence']))\n",
    "    \n",
    "    # If the difference is too small, skip this group\n",
    "    if diff < 2:  # Define your threshold\n",
    "        continue\n",
    "    \n",
    "    # Create a new row with the 'GPT4V_response' and the selected candidate response\n",
    "    new_row = {\n",
    "        'image': image,\n",
    "        'GPT4V_question_and_response': gpt4v_response['conversations'],  # Add 'conversations' column\n",
    "        'GPT4V_response': gpt4v_response['Output'],\n",
    "        'Candidate_question_and_response': candidate_response['conversations'],  # Add 'conversations' column\n",
    "        'Candidate_response': candidate_response['Output'],\n",
    "        'Helpfulness_gpt4v': gpt4v_response['Helpfulness'],\n",
    "        'Correctness_gpt4v': gpt4v_response['Correctness'],\n",
    "        'Coherence_gpt4v': gpt4v_response['Coherence'],\n",
    "        'Sum_of_helpfulness_correctness_coherence_gpt4v': gpt4v_response['Helpfulness'] + gpt4v_response['Correctness'] + gpt4v_response['Coherence'],\n",
    "        'Complexity_gpt4v': gpt4v_response['Complexity'],\n",
    "        'Verbosity_gpt4v': gpt4v_response['Verbosity'],\n",
    "        'Helpfulness_candidate': candidate_response['Helpfulness'],\n",
    "        'Correctness_candidate': candidate_response['Correctness'],\n",
    "        'Coherence_candidate': candidate_response['Coherence'],\n",
    "        'Sum_of_helpfulness_correctness_coherence_candidate': candidate_response['Helpfulness'] + candidate_response['Correctness'] + candidate_response['Coherence'],\n",
    "        'Complexity_candidate': candidate_response['Complexity'],\n",
    "        'Verbosity_candidate': candidate_response['Verbosity'],\n",
    "    }\n",
    "    \n",
    "    new_rows.append(new_row)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "sum_attributes = ['Sum_of_helpfulness_correctness_coherence_gpt4v', \n",
    " 'Sum_of_helpfulness_correctness_coherence_candidate']\n",
    "new_df[sum_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def display(img_filename):\n",
    "    img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image \n",
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def display(img_filename):\n",
    "    img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4V_question and Candidate_question are not equal, filtering\n",
      "Number of rows before filtering:  4587\n",
      "Number of rows after filtering:  4383\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Union, Optional\n",
    "def extract_question_turn(conversations: List[dict]) -> dict:\n",
    "    \"\"\"Extract the question turn from a list of turns in a conversation.\"\"\"\n",
    "    question_turn = conversations[0]\n",
    "    assert question_turn['from'] == 'human'\n",
    "    question_str = question_turn['value']\n",
    "    if '\\nPlease answer' in question_str:\n",
    "        question_str = question_str.split('\\nPlease answer')[0]\n",
    "    return {'from': 'human', 'value': question_str}\n",
    "\n",
    "\n",
    "def extract_answer_turn_to_match_Output(conversations: List[dict]) -> dict:\n",
    "    \"\"\"Extract the question turn from a list of turns in a conversation.\"\"\"\n",
    "    answer_turn = conversations[1]\n",
    "    assert answer_turn['from'] == 'gpt'\n",
    "    return answer_turn['value']\n",
    "assert new_df['GPT4V_question_and_response'].apply(lambda x: extract_answer_turn_to_match_Output(x)).equals(new_df['GPT4V_response'])\n",
    "assert new_df['Candidate_question_and_response'].apply(lambda x: extract_answer_turn_to_match_Output(x)).equals(new_df['Candidate_response'])\n",
    "\n",
    "new_df['GPT4V_question'] = new_df['GPT4V_question_and_response'].apply(lambda x: extract_question_turn(x))\n",
    "new_df['Candidate_question'] = new_df['Candidate_question_and_response'].apply(lambda x: extract_question_turn(x))\n",
    "if not new_df['GPT4V_question'].equals(new_df['Candidate_question']): \n",
    "    print('GPT4V_question and Candidate_question are not equal, filtering')\n",
    "    print('Number of rows before filtering: ', len(new_df))\n",
    "    new_df = new_df[(new_df['GPT4V_question']) == (new_df['Candidate_question'])]\n",
    "    print('Number of rows after filtering: ', len(new_df))\n",
    "\n",
    "new_df['question'] = new_df['GPT4V_question']\n",
    "del new_df['GPT4V_question']\n",
    "del new_df['Candidate_question']\n",
    "\n",
    "new_df['chosen'] = new_df['GPT4V_response']\n",
    "new_df['rejected'] = new_df['Candidate_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_csv = '/home/ubuntu/latest_llava/LLaVA/playground/data/dpo/interleaf6k_filtered__4406images_atleast2_in_sum_quality_for_dpo_inference.csv'\n",
    "\n",
    "new_df.to_csv(intermediate_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "from PIL import Image\n",
    "import math\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/latest_llava/LLaVA/checkpoints/llava-v1.5-7b-lora-pure-sft-1.5lrv-2kllava-1230-merged'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)\n",
    "\n",
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "IMAGE_FOLDER=\"/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017/\"\n",
    "inference_data_path = intermediate_csv\n",
    "output_path = intermediate_csv.split('csv')[0] + 'with_logp.json'\n",
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 tokenizer,\n",
    "                 img_processor,\n",
    "                 use_im_start_end):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "\n",
    "        self.mm_cfg = {\n",
    "            'image_processor': img_processor,\n",
    "            'is_multimodal': True,\n",
    "            # 'image_token_len': image_token_len, # TODO check if needed\n",
    "            'use_im_start_end': use_im_start_end,\n",
    "            'image_aspect_ratio': 'pad'\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # After encode multimodal preference sample, \n",
    "        # the image would ahve the pixel values values\n",
    "        sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "        rej_data_dict, win_data_dict = encode_multimodal_preference_sample(sample, self.tokenizer, self.mm_cfg)\n",
    "        return rej_data_dict, win_data_dict\n",
    "    \n",
    "    def _get_image(self, img_filename):\n",
    "        img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def _convert_to_llava_answer_turn(self, answer):\n",
    "        return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n",
    "    def convert_dataframe_row_to_source(self, row):\n",
    "        dict_output = {'question': row['question'], \n",
    "                       # Chosen and rejected based on the sum of the first three quality score\n",
    "                    'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "                    'rejected': self._convert_to_llava_answer_turn(row['rejected']),\n",
    "                    'image': self._get_image(row['image'])}\n",
    "        return dict_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset(inference_data_path,\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]\n",
    "\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')\n",
    "\n",
    "df = pd.read_csv(inference_data_path)\n",
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "# Add each list as a column to the dataframe with its variable name\n",
    "df['win_logp'] = win_logp_list\n",
    "df['rej_logp'] = rej_logp_list\n",
    "df['win_avg_logp'] = win_avg_logp_list\n",
    "df['rej_avg_logp'] = rej_avg_logp_list\n",
    "df['win_per_token_logp'] = win_per_token_logp_list\n",
    "df['rej_per_token_logp'] = rej_per_token_logp_list\n",
    "exisitng_columns = ['rej_logp', \n",
    "                    'win_logp',\n",
    "                    'rej_avg_logp',\n",
    "                    'win_avg_logp',\n",
    "                    'rej_per_token_logp',\n",
    "                    'win_per_token_logp']\n",
    "for col in exisitng_columns:\n",
    "    rename = 'ref_' + col\n",
    "    df[rename] = df[col]\n",
    "    del df[col]\n",
    "df['chosen'] = df['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "df['rejected'] = df['rejected'].apply(_convert_to_llava_answer_turn)\n",
    "# df.to_json(output_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/latest_llava/LLaVA/playground/data/dpo/interleaf6k_filtered__4406images_atleast2_in_sum_quality_for_dpo_inference.with_logp.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(output_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IMAGE_FOLDER=/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017/000000000092.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpreference_torch_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 52\u001b[0m, in \u001b[0;36mPreferenceInferenceDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# After encode multimodal preference sample, \u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# the image would ahve the pixel values values\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dataframe_row_to_source\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     rej_data_dict, win_data_dict \u001b[38;5;241m=\u001b[39m encode_multimodal_preference_sample(sample, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmm_cfg)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rej_data_dict, win_data_dict\n",
      "Cell \u001b[0;32mIn[16], line 70\u001b[0m, in \u001b[0;36mPreferenceInferenceDataset.convert_dataframe_row_to_source\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_dataframe_row_to_source\u001b[39m(\u001b[38;5;28mself\u001b[39m, row):\n\u001b[1;32m     66\u001b[0m     dict_output \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     67\u001b[0m                    \u001b[38;5;66;03m# Chosen and rejected based on the sum of the first three quality score\u001b[39;00m\n\u001b[1;32m     68\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_llava_answer_turn(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_llava_answer_turn(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m---> 70\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dict_output\n",
      "Cell \u001b[0;32mIn[16], line 58\u001b[0m, in \u001b[0;36mPreferenceInferenceDataset._get_image\u001b[0;34m(self, img_filename)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_filename):\n\u001b[1;32m     57\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(IMAGE_FOLDER, img_filename)\n\u001b[0;32m---> 58\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IMAGE_FOLDER=/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017/000000000092.jpg'"
     ]
    }
   ],
   "source": [
    "next(iter(preference_torch_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_csv = '/home/ubuntu/LLaVA/playground/data/dpo/interleaf6k_filtered__4406images_atleast2_in_sum_quality_for_dpo_inference.csv'\n",
    "inference_data_path = intermediate_csv\n",
    "output_path = intermediate_csv.split('csv')[0] + 'with_logp.json'\n",
    "df_reload = pd.read_json(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'llava 1.5 13b log p for sampled response and gpt4v response')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_reload[['ref_rej_logp', 'ref_win_logp']].boxplot()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('llava 1.5 13b log p for sampled response and gpt4v response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samee code but for extracting log prob from DPO'ed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Loading LoRA weights...\n",
      "Merging LoRA weights...\n",
      "Model is loaded...\n",
      ".....Loading vision tower and image processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4406/4406 [09:14<00:00,  7.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "from PIL import Image\n",
    "import math\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/LLaVA/checkpoints/llava-v1.5-13b-steer-lora-helpsteerformat-interleaf-lrv3067llava3067-likert-1222_interleavedgpt4v_responseonly-pure'\n",
    "model_base = '/home/ubuntu/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, model_base, model_name)\n",
    "\n",
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n",
    "inference_data_path = intermediate_csv\n",
    "output_path = intermediate_csv.split('csv')[0] + 'with_logp.json'\n",
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 tokenizer,\n",
    "                 img_processor,\n",
    "                 use_im_start_end):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "\n",
    "        self.mm_cfg = {\n",
    "            'image_processor': img_processor,\n",
    "            'is_multimodal': True,\n",
    "            # 'image_token_len': image_token_len, # TODO check if needed\n",
    "            'use_im_start_end': use_im_start_end,\n",
    "            'image_aspect_ratio': 'pad'\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # After encode multimodal preference sample, \n",
    "        # the image would ahve the pixel values values\n",
    "        sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "        rej_data_dict, win_data_dict = encode_multimodal_preference_sample(sample, self.tokenizer, self.mm_cfg)\n",
    "        return rej_data_dict, win_data_dict\n",
    "    \n",
    "    def _get_image(self, img_filename):\n",
    "        img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def _convert_to_llava_answer_turn(self, answer):\n",
    "        return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n",
    "    def convert_dataframe_row_to_source(self, row):\n",
    "        dict_output = {'question': row['question'], \n",
    "                       # Chosen and rejected based on the sum of the first three quality score\n",
    "                    'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "                    'rejected': self._convert_to_llava_answer_turn(row['rejected']),\n",
    "                    'image': self._get_image(row['image'])}\n",
    "        return dict_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset(inference_data_path,\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]\n",
    "\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')\n",
    "\n",
    "df = pd.read_csv(inference_data_path)\n",
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "# Add each list as a column to the dataframe with its variable name\n",
    "df['win_logp'] = win_logp_list\n",
    "df['rej_logp'] = rej_logp_list\n",
    "df['win_avg_logp'] = win_avg_logp_list\n",
    "df['rej_avg_logp'] = rej_avg_logp_list\n",
    "df['win_per_token_logp'] = win_per_token_logp_list\n",
    "df['rej_per_token_logp'] = rej_per_token_logp_list\n",
    "exisitng_columns = ['rej_logp', \n",
    "                    'win_logp',\n",
    "                    'rej_avg_logp',\n",
    "                    'win_avg_logp',\n",
    "                    'rej_per_token_logp',\n",
    "                    'win_per_token_logp']\n",
    "for col in exisitng_columns:\n",
    "    rename = 'ref_' + col\n",
    "    df[rename] = df[col]\n",
    "    del df[col]\n",
    "df['chosen'] = df['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "df['rejected'] = df['rejected'].apply(_convert_to_llava_answer_turn)\n",
    "# df.to_json(output_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[['ref_rej_logp', 'ref_win_logp']].boxplot()\n",
    "plt.title('llava 1.5 13b PPO based on gpt4v response log p for self-sampled responses')\n",
    "plt.ylim(-500, 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old draft below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Loading vision tower and image processor...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect previous setup\n",
    "\n",
    "df_muffin_tsv = '/home/ubuntu/muffin/data/RLHF-V-Hall_v0/RLHF-V-Hall_v0-1837.tsv'\n",
    "df = pd.read_csv(df_muffin_tsv, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "          21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "            322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "          29889,  3148,  1001, 29901,   529,  3027, 29958,    13, 11008,   338,\n",
       "            445,  6808,   814,  5929, 12818,   304,  4856,   411,   263, 14225,\n",
       "            304,   720, 29973,   319,  1799,  9047, 13566, 29901,   450,  6808,\n",
       "            814,   297,   278,  1967, 29892,   607,  5680,   263,  8424,   310,\n",
       "            274,  1296, 10664,   297,   377, 16242,   907,   314,   322,   521,\n",
       "            542, 23167,   528,   485,   886, 29892,   723,   367, 10712,  5929,\n",
       "          12818,   304,  4856,   411,   263, 14225,   304,   720, 29889,   910,\n",
       "            338,  1363,   278,   274,  1296,  3528,  5692,   304,   367,   263,\n",
       "           8261,   322, 29156,  5362,  7539, 29892,  1754,  1584,   901,   875,\n",
       "          18499,   491,   278,  6124,   310,   278,   377, 16242,   907,   314,\n",
       "            322,   521,   542, 23167,   528,   485,   886, 29889,   450, 10296,\n",
       "            310, 21054,   943,   322,  1426,  1973, 29892,  1316,   408,   278,\n",
       "          14225,   322,   907, 25934,   377, 16242,   907,   314,   322,   278,\n",
       "          12814,   292,  2586,   725,   404,   310,   278,   521,   542, 23167,\n",
       "          29892,   723,  5517,   367,  1407, 24064,   363,  4856,  1058, 11418,\n",
       "            952, 29156,  3460,   297, 14225,  2578,  1446, 29889,   450, 24329,\n",
       "            310,   278,  6808,   814,   373,   278,  4796, 15284,   411,   263,\n",
       "          13283, 27350,   884, 12778,   304,   278, 12463, 25530, 29892,  3907,\n",
       "            372,   385,  3805,   690,   391,  1821,  2984,   363,  4856,   411,\n",
       "            263, 14225,   304,   720, 29889,     2]),\n",
       "  'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   450,  6808,\n",
       "            814,   297,   278,  1967, 29892,   607,  5680,   263,  8424,   310,\n",
       "            274,  1296, 10664,   297,   377, 16242,   907,   314,   322,   521,\n",
       "            542, 23167,   528,   485,   886, 29892,   723,   367, 10712,  5929,\n",
       "          12818,   304,  4856,   411,   263, 14225,   304,   720, 29889,   910,\n",
       "            338,  1363,   278,   274,  1296,  3528,  5692,   304,   367,   263,\n",
       "           8261,   322, 29156,  5362,  7539, 29892,  1754,  1584,   901,   875,\n",
       "          18499,   491,   278,  6124,   310,   278,   377, 16242,   907,   314,\n",
       "            322,   521,   542, 23167,   528,   485,   886, 29889,   450, 10296,\n",
       "            310, 21054,   943,   322,  1426,  1973, 29892,  1316,   408,   278,\n",
       "          14225,   322,   907, 25934,   377, 16242,   907,   314,   322,   278,\n",
       "          12814,   292,  2586,   725,   404,   310,   278,   521,   542, 23167,\n",
       "          29892,   723,  5517,   367,  1407, 24064,   363,  4856,  1058, 11418,\n",
       "            952, 29156,  3460,   297, 14225,  2578,  1446, 29889,   450, 24329,\n",
       "            310,   278,  6808,   814,   373,   278,  4796, 15284,   411,   263,\n",
       "          13283, 27350,   884, 12778,   304,   278, 12463, 25530, 29892,  3907,\n",
       "            372,   385,  3805,   690,   391,  1821,  2984,   363,  4856,   411,\n",
       "            263, 14225,   304,   720, 29889,     2]),\n",
       "  'image': tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           ...,\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],\n",
       "  \n",
       "          [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           ...,\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
       "  \n",
       "          [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           ...,\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])},\n",
       " {'input_ids': tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "          21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "            322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "          29889,  3148,  1001, 29901,   529,  3027, 29958,    13, 11008,   338,\n",
       "            445,  6808,   814,  5929, 12818,   304,  4856,   411,   263, 14225,\n",
       "            304,   720, 29973,   319,  1799,  9047, 13566, 29901,   910,  6808,\n",
       "            814, 29915, 29879, 25530,   380,  1567,   515,   967,  2473, 29899,\n",
       "          13148,   287,  3829, 29892,   411,  1269,  7546,  3063,  2730,   391,\n",
       "            322,  1602,   328,   296, 29892,  2504,  5921,   385, 17818,   344,\n",
       "            521,   542, 23167, 21054,   272,   393, 29915, 29879,   263, 22037,\n",
       "          25448,  4249,  1906,   411,   263, 14225,   304,   720, 29889,   450,\n",
       "           3144,  2209, 29891,   521,   542, 23167,  9581,  1829,   373,  2246,\n",
       "          29892,   411,   967, 10597, 18459,   322,  8261, 21779, 29892, 12778,\n",
       "            385,  4805,  7546,   310, 29156, 10238, 29889, 12808, 29892,   278,\n",
       "           7689,   682,  1847,   310,  4796,   521,   542, 23167, 17422, 10794,\n",
       "           8128,   451,   871,   263,  1998,  1474,  5929, 12818, 12814,   541,\n",
       "            884,   385,  5684, 14225,   322,   907, 25934, 21054,   272,  1543,\n",
       "            393, 11000,  1532,   411,   278,  6501,   521,   542, 23167, 29889,\n",
       "            450, 12463,   263,   342, 29882,  7492,   310, 28539,   368,  5096,\n",
       "            287, 15359,   322,   278,  5941, 24329,   373,   263,  2560,  4796,\n",
       "          15284,   411,   263, 27350,  7960,   304,   270,   573,   297,  3732,\n",
       "            278,  6808,   814,   451,   871, 13394, 13405,   292,   541,   884,\n",
       "          17739, 29899, 12554, 29891, 29892,   607,   508,   367, 10734,  5929,\n",
       "          12818,   297,  9826, 29915, 29879,  5264,  5745, 29899, 24477,   854,\n",
       "           9257, 29889,     2]),\n",
       "  'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   910,  6808,\n",
       "            814, 29915, 29879, 25530,   380,  1567,   515,   967,  2473, 29899,\n",
       "          13148,   287,  3829, 29892,   411,  1269,  7546,  3063,  2730,   391,\n",
       "            322,  1602,   328,   296, 29892,  2504,  5921,   385, 17818,   344,\n",
       "            521,   542, 23167, 21054,   272,   393, 29915, 29879,   263, 22037,\n",
       "          25448,  4249,  1906,   411,   263, 14225,   304,   720, 29889,   450,\n",
       "           3144,  2209, 29891,   521,   542, 23167,  9581,  1829,   373,  2246,\n",
       "          29892,   411,   967, 10597, 18459,   322,  8261, 21779, 29892, 12778,\n",
       "            385,  4805,  7546,   310, 29156, 10238, 29889, 12808, 29892,   278,\n",
       "           7689,   682,  1847,   310,  4796,   521,   542, 23167, 17422, 10794,\n",
       "           8128,   451,   871,   263,  1998,  1474,  5929, 12818, 12814,   541,\n",
       "            884,   385,  5684, 14225,   322,   907, 25934, 21054,   272,  1543,\n",
       "            393, 11000,  1532,   411,   278,  6501,   521,   542, 23167, 29889,\n",
       "            450, 12463,   263,   342, 29882,  7492,   310, 28539,   368,  5096,\n",
       "            287, 15359,   322,   278,  5941, 24329,   373,   263,  2560,  4796,\n",
       "          15284,   411,   263, 27350,  7960,   304,   270,   573,   297,  3732,\n",
       "            278,  6808,   814,   451,   871, 13394, 13405,   292,   541,   884,\n",
       "          17739, 29899, 12554, 29891, 29892,   607,   508,   367, 10734,  5929,\n",
       "          12818,   297,  9826, 29915, 29879,  5264,  5745, 29899, 24477,   854,\n",
       "           9257, 29889,     2]),\n",
       "  'image': tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           ...,\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],\n",
       "  \n",
       "          [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           ...,\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
       "  \n",
       "          [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           ...,\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 tokenizer,\n",
    "                 img_processor,\n",
    "                 use_im_start_end):\n",
    "        self.data = pd.read_csv(data_path, sep='\\t')\n",
    "\n",
    "        self.mm_cfg = {\n",
    "            'image_processor': img_processor,\n",
    "            'is_multimodal': True,\n",
    "            # 'image_token_len': image_token_len, # TODO check if needed\n",
    "            'use_im_start_end': use_im_start_end,\n",
    "            'image_aspect_ratio': 'pad'\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # After encode multimodal preference sample, \n",
    "        # the image would ahve the pixel values values\n",
    "        sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "        rej_data_dict, win_data_dict = encode_multimodal_preference_sample(sample, self.tokenizer, self.mm_cfg)\n",
    "        return rej_data_dict, win_data_dict\n",
    "    \n",
    "    def _get_image(self, img_filename):\n",
    "        img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def _convert_to_llava_answer_turn(self, answer):\n",
    "        return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n",
    "    def convert_dataframe_row_to_source(self, row):\n",
    "        dict_output = {'question': row['question'], \n",
    "                       # Chosen and rejected based on the sum of the first three quality score\n",
    "                    'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "                    'rejected': self._convert_to_llava_answer_turn(row['rejected']),\n",
    "                    'image': self._get_image(row['image'])}\n",
    "        return dict_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset('/home/ubuntu/muffin/data/RLHF-V-Hall_v0/RLHF-V-Hall_v0-1837.tsv',\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pad'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_torch_dataset.mm_cfg['image_aspect_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 336, 336])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _convert_to_llava_answer_turn(self, answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([226])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_torch_dataset[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 92, in __getattr__\n    return self.data[item]\nKeyError: 'shape'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 808, in preference_collator_fn\n    rej_batch = SFT_collator_fn(rej_instances, pad_token_id)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in SFT_collator_fn\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in <genexpr>\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 94, in __getattr__\n    raise AttributeError\nAttributeError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a7572655f6e6f6e2d73706f74227d/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataloader))\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 92, in __getattr__\n    return self.data[item]\nKeyError: 'shape'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 808, in preference_collator_fn\n    rej_batch = SFT_collator_fn(rej_instances, pad_token_id)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in SFT_collator_fn\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in <genexpr>\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 94, in __getattr__\n    raise AttributeError\nAttributeError\n"
     ]
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/_utils.py\u001b[0m(644)\u001b[0;36mreraise\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    642 \u001b[0;31m            \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    643 \u001b[0;31m            \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 644 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    645 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    646 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'images' is not defined\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(1371)\u001b[0;36m_process_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1369 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1370 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1371 \u001b[0;31m            \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1372 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1373 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'images' is not defined\n",
      "*** Invalid frame count (')\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(1345)\u001b[0;36m_next_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1343 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1344 \u001b[0;31m                \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1345 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1346 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1347 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(633)\u001b[0;36m__next__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    631 \u001b[0;31m                \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    632 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 633 \u001b[0;31m            \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    634 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    635 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
