{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Preload the interleaf data \n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nvidia/HelpSteer\")\n",
    "\n",
    "train = ds['train'] # len(train) = 35331 (95%)\n",
    "df = train.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34657, 35331)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby(['prompt'])\n",
    "filtered_df = grouped.filter(lambda x: len(x) >= 2)\n",
    "len(filtered_df), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Output'] = df['response']\n",
    "del df['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns=lambda x: x.capitalize(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111594/3407610649.py:8: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for image, group in df.groupby(['Prompt']):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sum_of_helpfulness_correctness_coherence_gpt4v</th>\n",
       "      <th>Sum_of_helpfulness_correctness_coherence_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7533.000000</td>\n",
       "      <td>7533.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.758396</td>\n",
       "      <td>7.150272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.454674</td>\n",
       "      <td>2.329527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sum_of_helpfulness_correctness_coherence_gpt4v  \\\n",
       "count                                     7533.000000   \n",
       "mean                                        10.758396   \n",
       "std                                          1.454674   \n",
       "min                                          2.000000   \n",
       "25%                                         10.000000   \n",
       "50%                                         11.000000   \n",
       "75%                                         12.000000   \n",
       "max                                         12.000000   \n",
       "\n",
       "       Sum_of_helpfulness_correctness_coherence_candidate  \n",
       "count                                        7533.000000   \n",
       "mean                                            7.150272   \n",
       "std                                             2.329527   \n",
       "min                                             0.000000   \n",
       "25%                                             6.000000   \n",
       "50%                                             8.000000   \n",
       "75%                                             9.000000   \n",
       "max                                            10.000000   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your original DataFrame\n",
    "new_rows = []\n",
    "\n",
    "# Group the DataFrame by 'image'\n",
    "for image, group in df.groupby(['Prompt']):\n",
    "    # Find the 'GPT4V_response' and candidate responses within this group\n",
    "    candidate_responses = group\n",
    "    # If there are no candidate responses, skip this group\n",
    "    if candidate_responses.empty:\n",
    "        continue\n",
    "    \n",
    "    # Calculate the difference in the total sum of 'Helpfulness', 'Correctness', and 'Coherence'\n",
    "    candidate_responses['Sum_quality'] = candidate_responses['Helpfulness'] + candidate_responses['Correctness'] + candidate_responses['Coherence']\n",
    "    diff = abs(max(candidate_responses['Sum_quality']) - min(candidate_responses['Sum_quality']))\n",
    "    \n",
    "    # If the difference is too small, skip this group\n",
    "    if diff < 2:  # Define your threshold\n",
    "        continue\n",
    "    \n",
    "    # Find the response with the highest 'Sum_quality'\n",
    "    max_row_index = candidate_responses['Sum_quality'].idxmax()\n",
    "    max_row = candidate_responses.loc[max_row_index]\n",
    "    \n",
    "    # Filter for responses that are at least 2 lesser or equal\n",
    "    filtered_responses = candidate_responses[candidate_responses['Sum_quality'] <= max_row['Sum_quality'] - 2]\n",
    "    \n",
    "    # If the filtered group is empty, skip this group\n",
    "    if filtered_responses.empty:\n",
    "        continue\n",
    "    \n",
    "    # Randomly sample one response from the filtered group\n",
    "    selected_row = filtered_responses.sample(n=1)\n",
    "    # Create a new row with the 'GPT4V_response' and the selected candidate response\n",
    "    new_row = {\n",
    "        'question': max_row['Prompt'],\n",
    "        # 'GPT4V_question_and_response': max_row['conversations'],  # Add 'conversations' column\n",
    "        'chosen': max_row['Output'],\n",
    "        # 'Candidate_question_and_response': selected_row['conversations'],  # Add 'conversations' column\n",
    "        'rejected': selected_row['Output'].item(),\n",
    "        'Helpfulness_gpt4v': max_row['Helpfulness'],\n",
    "        'Correctness_gpt4v': max_row['Correctness'],\n",
    "        'Coherence_gpt4v': max_row['Coherence'],\n",
    "        'Sum_of_helpfulness_correctness_coherence_gpt4v': max_row['Helpfulness'] + max_row['Correctness'] + max_row['Coherence'],\n",
    "        'Complexity_gpt4v': max_row['Complexity'],\n",
    "        'Verbosity_gpt4v': max_row['Verbosity'],\n",
    "        'Helpfulness_candidate': selected_row['Helpfulness'].values[0],\n",
    "        'Correctness_candidate': selected_row['Correctness'].values[0],\n",
    "        'Coherence_candidate': selected_row['Coherence'].values[0],\n",
    "        'Sum_of_helpfulness_correctness_coherence_candidate': selected_row['Helpfulness'].values[0] + selected_row['Correctness'].values[0] + selected_row['Coherence'].values[0],\n",
    "        'Complexity_candidate': selected_row['Complexity'].values[0],\n",
    "        'Verbosity_candidate': selected_row['Verbosity'].values[0],\n",
    "    }\n",
    "    \n",
    "    new_rows.append(new_row)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "sum_attributes = ['Sum_of_helpfulness_correctness_coherence_gpt4v', \n",
    " 'Sum_of_helpfulness_correctness_coherence_candidate']\n",
    "new_df[sum_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7533, 35331)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def form_multi_turn(row):\n",
    "#     first_turn = {'from': 'human', 'value': row['question']}\n",
    "    \n",
    "#     first_candidate = {'from': 'gpt', 'value': row['chosen']}\n",
    "#     candidate_attribute_str = f'''The previous answer is in the following quality metrics in the 0-4 likert scale: Helpfulness: {row['Helpfulness_candidate']}, Correctness: {row['Correctness_candidate']}, Coherence: {row['Coherence_candidate']}, Complexity: {row['Complexity_candidate']}, Verbosity: {row['Verbosity_candidate']}.\n",
    "#     Please answer the question such that your response is in the following attributes in 5-point Likert scale: Helpfulness: {row['Helpfulness_gpt4v']}, Correctness: {row['Correctness_gpt4v']}, Coherence: {row['Coherence_gpt4v']}, Complexity: {row['Complexity_gpt4v']}, Verbosity: {row['Verbosity_gpt4v']}. '''\n",
    "#     preference = {'from': 'human', 'value': candidate_attribute_str}\n",
    "#     final_turn = {'from': 'gpt', 'value': row['rejected']}\n",
    "#     return [first_turn, first_candidate, preference, final_turn]\n",
    "\n",
    "\n",
    "# new_df['conversations'] = new_df.apply(form_multi_turn, axis=1)\n",
    "# new_df['image'] = 'scigraph/' + new_df['image'].astype(str) + '.jpg'\n",
    "# new_df.to_json('/home/ubuntu/latest_llava/LLaVA/playground/data/concatnateed_helpsteer/scigraph3000_self-sampling.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A group of people have been wrongfully convict...\n",
       "1       A scientist is working on a spectrograph to de...\n",
       "2       Act as a fraudster who has just been arrested ...\n",
       "3       Act as if you are a Buddhist monk who just fou...\n",
       "4       Act as if you are a codon in a gene, describin...\n",
       "                              ...                        \n",
       "7528    given this background information delimited in...\n",
       "7529    given this background information delimited in...\n",
       "7530    given this background information delimited in...\n",
       "7531    given this background information delimited in...\n",
       "7532    given this background information delimited in...\n",
       "Name: question, Length: 7533, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a group of people who have been wrongfully convicted of a crime, we would like to express our deepest apologies for any trouble we may have caused. We would like to take this opportunity to explain what happened and clear up any misconceptions that may have arisen.\\n\\nFirstly, we would like to clarify that we did not commit the crime that we were convicted of. We have always maintained our innocence and we continue to do so today. We understand that the justice system is not perfect and that mistakes can happen, but we do not believe that we are responsible for this crime.\\n\\nSecondly, we would like to apologize for any inconvenience or distress that we may have caused to the victims of this crime. We understand that they have been through a traumatic experience and we do not want our situation to cause any further pain or suffering. We hope that they can find closure and healing in the future.\\n\\nFinally, we would like to thank everyone who has supported us throughout this difficult time. We have received an overwhelming amount of support from friends, family, and strangers alike, and we are truly grateful for all of it. We will continue to fight for our innocence and we hope that one day we will be able to put this behind us and move forward with our lives.\\n\\nOnce again, we would like to apologize for any trouble we may have caused and we hope that we can put this situation behind us and move forward in a positive way.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['rejected'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My name is [Name], and I am one of the wrongfully convicted. I was wrongfully convicted for a crime that I did not commit, and I want to share my experience with you.\\n\\nIt all started when I was arrested and charged with a crime that I didn't commit. I was innocent, but the evidence against me was overwhelming. I was interrogated for hours, and I eventually confessed to a crime that I didn't commit.\\n\\nThe trial was a circus. The prosecution presented a case that was full of lies and deceit. The jury didn't believe me, and I was found guilty. I was sentenced to life in prison.\\n\\nFor years, I struggled to make sense of what happened to me. I was lost and confused, and I felt like the justice system had failed me. But then, I found out about [Organization], a group of people who are dedicated to helping those who have been wrongfully convicted.\\n\\nI reached out to [Organization], and they started working on my case. They investigated the evidence, and they found new witnesses who came forward with new information. They uncovered evidence that proved my innocence, and after years of fighting, I was finally freed.\\n\\nBeing wrongfully convicted was one of the worst experiences of my life. But thanks to [Organization], I was able to clear my name and start a new chapter. I want to share my story with you to raise awareness about the importance of fair trials and the need to protect the rights of the innocent.\\n\\nMy name is [Name], and I am one of the wrongfully convicted. But now, I am free.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['chosen'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important note: Randomly sample 6k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = new_df.sample(n=6000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the remove helpsteer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous = '/home/ubuntu/latest_llava/LLaVA/playground/data/helpsteer/intermediate_6k_sample.csv'\n",
    "df_previous = pd.read_csv(df_previous)\n",
    "new_df = new_df.drop(df_previous.index, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1533"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_csv = '/home/ubuntu/latest_llava/LLaVA/playground/data/dpo/0109_dpo_previous_1500llava_1500lrv/remaining_1533_helpsteer.csv'\n",
    "\n",
    "new_df.to_csv(intermediate_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Loading LoRA weights...\n",
      "Merging LoRA weights...\n",
      "Model is loaded...\n",
      ".....Loading vision tower and image processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 601/1533 [04:14<05:15,  2.96it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2134 > 2048). Running this sequence through the model will result in indexing errors\n",
      " 47%|████▋     | 719/1533 [05:07<07:37,  1.78it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2203 > 2048). Running this sequence through the model will result in indexing errors\n",
      " 67%|██████▋   | 1028/1533 [07:28<03:36,  2.33it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2112 > 2048). Running this sequence through the model will result in indexing errors\n",
      " 87%|████████▋ | 1332/1533 [09:50<01:11,  2.82it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2316 > 2048). Running this sequence through the model will result in indexing errors\n",
      " 97%|█████████▋| 1485/1533 [11:02<00:20,  2.36it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2252 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1533/1533 [11:25<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "from PIL import Image\n",
    "import math\n",
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "import torch.utils.data as torch_data\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/latest_llava/LLaVA/checkpoints/llava-v1.5-13b-to_lrv10k-13k-scigraph3k-helpsteer6k_0107_lora'\n",
    "model_base = '/home/ubuntu/latest_llava/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name)\n",
    "\n",
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "IMAGE_FOLDER=\"/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017/\"\n",
    "inference_data_path = intermediate_csv\n",
    "output_path = intermediate_csv.split('csv')[0] + 'with_logp.json'\n",
    "\n",
    "class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 tokenizer,\n",
    "                 img_processor,\n",
    "                 use_im_start_end):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "\n",
    "        self.mm_cfg = {\n",
    "            'image_processor': img_processor,\n",
    "            'is_multimodal': True,\n",
    "            # 'image_token_len': image_token_len, # TODO check if needed\n",
    "            'use_im_start_end': use_im_start_end,\n",
    "            'image_aspect_ratio': 'pad'\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # After encode multimodal preference sample, \n",
    "        # the image would ahve the pixel values values\n",
    "        sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "        rej_data_dict, win_data_dict = encode_multimodal_preference_sample(sample, self.tokenizer, self.mm_cfg)\n",
    "        return rej_data_dict, win_data_dict\n",
    "    \n",
    "    def _get_image(self, img_filename):\n",
    "        img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def _convert_to_llava_answer_turn(self, answer):\n",
    "        if not isinstance(answer, str):\n",
    "            answer = str(answer)\n",
    "        return {\"from\": \"gpt\", \"value\": answer}\n",
    "    \n",
    "    def _convert_to_llava_question_turn(self, question):\n",
    "        if not isinstance(question, str):\n",
    "            question = str(question)\n",
    "        return {\"from\": \"human\", \"value\": question}\n",
    "\n",
    "    def convert_dataframe_row_to_source(self, row):\n",
    "        dict_output = {'question': self._convert_to_llava_question_turn(row['question']), \n",
    "                       # Chosen and rejected based on the sum of the first three quality score\n",
    "                    'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "                    'rejected': self._convert_to_llava_answer_turn(row['rejected'])}\n",
    "        return dict_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset(inference_data_path,\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]\n",
    "\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)] # Previously commented out\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')\n",
    "\n",
    "df = pd.read_csv(inference_data_path)\n",
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "# Add each list as a column to the dataframe with its variable name\n",
    "df['win_logp'] = win_logp_list\n",
    "df['rej_logp'] = rej_logp_list\n",
    "df['win_avg_logp'] = win_avg_logp_list\n",
    "df['rej_avg_logp'] = rej_avg_logp_list\n",
    "df['win_per_token_logp'] = win_per_token_logp_list\n",
    "df['rej_per_token_logp'] = rej_per_token_logp_list\n",
    "exisitng_columns = ['rej_logp', \n",
    "                    'win_logp',\n",
    "                    'rej_avg_logp',\n",
    "                    'win_avg_logp',\n",
    "                    'rej_per_token_logp',\n",
    "                    'win_per_token_logp']\n",
    "for col in exisitng_columns:\n",
    "    rename = 'ref_' + col\n",
    "    df[rename] = df[col]\n",
    "    del df[col]\n",
    "df['chosen'] = df['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "df['rejected'] = df['rejected'].apply(_convert_to_llava_answer_turn)\n",
    "def _convert_to_llava_question_turn(question):\n",
    "    return {\"from\": \"human\", \"value\": question}\n",
    "df['question'] = df['question'].apply(_convert_to_llava_question_turn)\n",
    "df.to_json(output_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/latest_llava/LLaVA/playground/data/dpo/0109_dpo_previous_1500llava_1500lrv/remaining_1533_helpsteer.with_logp.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df[['ref_rej_logp', 'ref_win_logp']].boxplot()\n",
    "\n",
    "plt.title('13b-SFT-checkpoint Log Probability on preference dataset before DPO', fontsize=10)\n",
    "\n",
    "# Set the x-ticks\n",
    "plt.xticks([1, 2], ['Rejected Responses', 'Preferred Reponses'])\n",
    "plt.ylabel('Log Probability', fontsize=12)\n",
    "\n",
    "# Save the plot in high DPI and PDF format\n",
    "#\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_reload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_reload\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(_convert_to_llava_answer_turn)\n\u001b[1;32m      2\u001b[0m df_reload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_reload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(_convert_to_llava_answer_turn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_reload' is not defined"
     ]
    }
   ],
   "source": [
    "df_reload['chosen'] = df_reload['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "df_reload['rejected'] = df_reload['rejected'].apply(_convert_to_llava_answer_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_reload\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_reload' is not defined"
     ]
    }
   ],
   "source": [
    "df_reload['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('/home/ubuntu/LLaVA/playground/data/dpo/dpo_logp.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old draft below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Loading vision tower and image processor...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect previous setup\n",
    "\n",
    "df_muffin_tsv = '/home/ubuntu/muffin/data/RLHF-V-Hall_v0/RLHF-V-Hall_v0-1837.tsv'\n",
    "df = pd.read_csv(df_muffin_tsv, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "          21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "            322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "          29889,  3148,  1001, 29901,   529,  3027, 29958,    13, 11008,   338,\n",
       "            445,  6808,   814,  5929, 12818,   304,  4856,   411,   263, 14225,\n",
       "            304,   720, 29973,   319,  1799,  9047, 13566, 29901,   450,  6808,\n",
       "            814,   297,   278,  1967, 29892,   607,  5680,   263,  8424,   310,\n",
       "            274,  1296, 10664,   297,   377, 16242,   907,   314,   322,   521,\n",
       "            542, 23167,   528,   485,   886, 29892,   723,   367, 10712,  5929,\n",
       "          12818,   304,  4856,   411,   263, 14225,   304,   720, 29889,   910,\n",
       "            338,  1363,   278,   274,  1296,  3528,  5692,   304,   367,   263,\n",
       "           8261,   322, 29156,  5362,  7539, 29892,  1754,  1584,   901,   875,\n",
       "          18499,   491,   278,  6124,   310,   278,   377, 16242,   907,   314,\n",
       "            322,   521,   542, 23167,   528,   485,   886, 29889,   450, 10296,\n",
       "            310, 21054,   943,   322,  1426,  1973, 29892,  1316,   408,   278,\n",
       "          14225,   322,   907, 25934,   377, 16242,   907,   314,   322,   278,\n",
       "          12814,   292,  2586,   725,   404,   310,   278,   521,   542, 23167,\n",
       "          29892,   723,  5517,   367,  1407, 24064,   363,  4856,  1058, 11418,\n",
       "            952, 29156,  3460,   297, 14225,  2578,  1446, 29889,   450, 24329,\n",
       "            310,   278,  6808,   814,   373,   278,  4796, 15284,   411,   263,\n",
       "          13283, 27350,   884, 12778,   304,   278, 12463, 25530, 29892,  3907,\n",
       "            372,   385,  3805,   690,   391,  1821,  2984,   363,  4856,   411,\n",
       "            263, 14225,   304,   720, 29889,     2]),\n",
       "  'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   450,  6808,\n",
       "            814,   297,   278,  1967, 29892,   607,  5680,   263,  8424,   310,\n",
       "            274,  1296, 10664,   297,   377, 16242,   907,   314,   322,   521,\n",
       "            542, 23167,   528,   485,   886, 29892,   723,   367, 10712,  5929,\n",
       "          12818,   304,  4856,   411,   263, 14225,   304,   720, 29889,   910,\n",
       "            338,  1363,   278,   274,  1296,  3528,  5692,   304,   367,   263,\n",
       "           8261,   322, 29156,  5362,  7539, 29892,  1754,  1584,   901,   875,\n",
       "          18499,   491,   278,  6124,   310,   278,   377, 16242,   907,   314,\n",
       "            322,   521,   542, 23167,   528,   485,   886, 29889,   450, 10296,\n",
       "            310, 21054,   943,   322,  1426,  1973, 29892,  1316,   408,   278,\n",
       "          14225,   322,   907, 25934,   377, 16242,   907,   314,   322,   278,\n",
       "          12814,   292,  2586,   725,   404,   310,   278,   521,   542, 23167,\n",
       "          29892,   723,  5517,   367,  1407, 24064,   363,  4856,  1058, 11418,\n",
       "            952, 29156,  3460,   297, 14225,  2578,  1446, 29889,   450, 24329,\n",
       "            310,   278,  6808,   814,   373,   278,  4796, 15284,   411,   263,\n",
       "          13283, 27350,   884, 12778,   304,   278, 12463, 25530, 29892,  3907,\n",
       "            372,   385,  3805,   690,   391,  1821,  2984,   363,  4856,   411,\n",
       "            263, 14225,   304,   720, 29889,     2]),\n",
       "  'image': tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           ...,\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],\n",
       "  \n",
       "          [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           ...,\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
       "  \n",
       "          [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           ...,\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])},\n",
       " {'input_ids': tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "          21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "            322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "          29889,  3148,  1001, 29901,   529,  3027, 29958,    13, 11008,   338,\n",
       "            445,  6808,   814,  5929, 12818,   304,  4856,   411,   263, 14225,\n",
       "            304,   720, 29973,   319,  1799,  9047, 13566, 29901,   910,  6808,\n",
       "            814, 29915, 29879, 25530,   380,  1567,   515,   967,  2473, 29899,\n",
       "          13148,   287,  3829, 29892,   411,  1269,  7546,  3063,  2730,   391,\n",
       "            322,  1602,   328,   296, 29892,  2504,  5921,   385, 17818,   344,\n",
       "            521,   542, 23167, 21054,   272,   393, 29915, 29879,   263, 22037,\n",
       "          25448,  4249,  1906,   411,   263, 14225,   304,   720, 29889,   450,\n",
       "           3144,  2209, 29891,   521,   542, 23167,  9581,  1829,   373,  2246,\n",
       "          29892,   411,   967, 10597, 18459,   322,  8261, 21779, 29892, 12778,\n",
       "            385,  4805,  7546,   310, 29156, 10238, 29889, 12808, 29892,   278,\n",
       "           7689,   682,  1847,   310,  4796,   521,   542, 23167, 17422, 10794,\n",
       "           8128,   451,   871,   263,  1998,  1474,  5929, 12818, 12814,   541,\n",
       "            884,   385,  5684, 14225,   322,   907, 25934, 21054,   272,  1543,\n",
       "            393, 11000,  1532,   411,   278,  6501,   521,   542, 23167, 29889,\n",
       "            450, 12463,   263,   342, 29882,  7492,   310, 28539,   368,  5096,\n",
       "            287, 15359,   322,   278,  5941, 24329,   373,   263,  2560,  4796,\n",
       "          15284,   411,   263, 27350,  7960,   304,   270,   573,   297,  3732,\n",
       "            278,  6808,   814,   451,   871, 13394, 13405,   292,   541,   884,\n",
       "          17739, 29899, 12554, 29891, 29892,   607,   508,   367, 10734,  5929,\n",
       "          12818,   297,  9826, 29915, 29879,  5264,  5745, 29899, 24477,   854,\n",
       "           9257, 29889,     2]),\n",
       "  'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   910,  6808,\n",
       "            814, 29915, 29879, 25530,   380,  1567,   515,   967,  2473, 29899,\n",
       "          13148,   287,  3829, 29892,   411,  1269,  7546,  3063,  2730,   391,\n",
       "            322,  1602,   328,   296, 29892,  2504,  5921,   385, 17818,   344,\n",
       "            521,   542, 23167, 21054,   272,   393, 29915, 29879,   263, 22037,\n",
       "          25448,  4249,  1906,   411,   263, 14225,   304,   720, 29889,   450,\n",
       "           3144,  2209, 29891,   521,   542, 23167,  9581,  1829,   373,  2246,\n",
       "          29892,   411,   967, 10597, 18459,   322,  8261, 21779, 29892, 12778,\n",
       "            385,  4805,  7546,   310, 29156, 10238, 29889, 12808, 29892,   278,\n",
       "           7689,   682,  1847,   310,  4796,   521,   542, 23167, 17422, 10794,\n",
       "           8128,   451,   871,   263,  1998,  1474,  5929, 12818, 12814,   541,\n",
       "            884,   385,  5684, 14225,   322,   907, 25934, 21054,   272,  1543,\n",
       "            393, 11000,  1532,   411,   278,  6501,   521,   542, 23167, 29889,\n",
       "            450, 12463,   263,   342, 29882,  7492,   310, 28539,   368,  5096,\n",
       "            287, 15359,   322,   278,  5941, 24329,   373,   263,  2560,  4796,\n",
       "          15284,   411,   263, 27350,  7960,   304,   270,   573,   297,  3732,\n",
       "            278,  6808,   814,   451,   871, 13394, 13405,   292,   541,   884,\n",
       "          17739, 29899, 12554, 29891, 29892,   607,   508,   367, 10734,  5929,\n",
       "          12818,   297,  9826, 29915, 29879,  5264,  5745, 29899, 24477,   854,\n",
       "           9257, 29889,     2]),\n",
       "  'image': tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           ...,\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],\n",
       "  \n",
       "          [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           ...,\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
       "  \n",
       "          [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           ...,\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 tokenizer,\n",
    "                 img_processor,\n",
    "                 use_im_start_end):\n",
    "        self.data = pd.read_csv(data_path, sep='\\t')\n",
    "\n",
    "        self.mm_cfg = {\n",
    "            'image_processor': img_processor,\n",
    "            'is_multimodal': True,\n",
    "            # 'image_token_len': image_token_len, # TODO check if needed\n",
    "            'use_im_start_end': use_im_start_end,\n",
    "            'image_aspect_ratio': 'pad'\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # After encode multimodal preference sample, \n",
    "        # the image would ahve the pixel values values\n",
    "        sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "        rej_data_dict, win_data_dict = encode_multimodal_preference_sample(sample, self.tokenizer, self.mm_cfg)\n",
    "        return rej_data_dict, win_data_dict\n",
    "    \n",
    "    def _get_image(self, img_filename):\n",
    "        img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def _convert_to_llava_answer_turn(self, answer):\n",
    "        return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n",
    "    def convert_dataframe_row_to_source(self, row):\n",
    "        dict_output = {'question': row['question'], \n",
    "                       # Chosen and rejected based on the sum of the first three quality score\n",
    "                    'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "                    'rejected': self._convert_to_llava_answer_turn(row['rejected']),\n",
    "                    'image': self._get_image(row['image'])}\n",
    "        return dict_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset('/home/ubuntu/muffin/data/RLHF-V-Hall_v0/RLHF-V-Hall_v0-1837.tsv',\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pad'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_torch_dataset.mm_cfg['image_aspect_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 336, 336])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _convert_to_llava_answer_turn(self, answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([226])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_torch_dataset[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 92, in __getattr__\n    return self.data[item]\nKeyError: 'shape'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 808, in preference_collator_fn\n    rej_batch = SFT_collator_fn(rej_instances, pad_token_id)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in SFT_collator_fn\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in <genexpr>\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 94, in __getattr__\n    raise AttributeError\nAttributeError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a7572655f6e6f6e2d73706f74227d/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataloader))\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 92, in __getattr__\n    return self.data[item]\nKeyError: 'shape'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 808, in preference_collator_fn\n    rej_batch = SFT_collator_fn(rej_instances, pad_token_id)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in SFT_collator_fn\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in <genexpr>\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 94, in __getattr__\n    raise AttributeError\nAttributeError\n"
     ]
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/_utils.py\u001b[0m(644)\u001b[0;36mreraise\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    642 \u001b[0;31m            \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    643 \u001b[0;31m            \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 644 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    645 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    646 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'images' is not defined\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(1371)\u001b[0;36m_process_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1369 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1370 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1371 \u001b[0;31m            \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1372 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1373 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'images' is not defined\n",
      "*** Invalid frame count (')\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(1345)\u001b[0;36m_next_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1343 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1344 \u001b[0;31m                \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1345 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1346 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1347 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(633)\u001b[0;36m__next__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    631 \u001b[0;31m                \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    632 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 633 \u001b[0;31m            \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    634 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    635 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
