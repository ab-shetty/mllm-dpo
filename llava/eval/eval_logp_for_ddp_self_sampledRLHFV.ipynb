{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload the interleaf data \n",
    "import pandas as pd\n",
    "df = pd.read_csv('/home/ubuntu/LLaVA/playground/data/dpo/dpo_logp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image \n",
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def display(img_filename):\n",
    "    img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Loading vision tower and image processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1401/1401 [02:45<00:00,  8.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "import base64\n",
    "import ast\n",
    "import math\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, model_name)\n",
    "\n",
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n",
    "intermediate_csv = '/home/ubuntu/LLaVA/playground/data/dpo/dpo_rlhf_imagepath.csv'\n",
    "inference_data_path = intermediate_csv\n",
    "output_path = intermediate_csv.split('csv')[0] + 'with_logp.json'\n",
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 tokenizer,\n",
    "                 img_processor,\n",
    "                 use_im_start_end):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        columns_toconvert = ['question', 'chosen', 'rejected']\n",
    "        for col in columns_toconvert:\n",
    "            self.data[col] = self.data[col].apply(lambda x: ast.literal_eval(x))\n",
    "        self.mm_cfg = {\n",
    "            'image_processor': img_processor,\n",
    "            'is_multimodal': True,\n",
    "            # 'image_token_len': image_token_len, # TODO check if needed\n",
    "            'use_im_start_end': use_im_start_end,\n",
    "            'image_aspect_ratio': 'pad',\n",
    "            \"image_folder\": IMAGE_FOLDER,\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # After encode multimodal preference sample, \n",
    "        # the image would ahve the pixel values values\n",
    "        sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "        rej_data_dict, win_data_dict = encode_multimodal_preference_sample(self.data.iloc[index], self.tokenizer, self.mm_cfg)\n",
    "        return rej_data_dict, win_data_dict\n",
    "    \n",
    "    def _get_image(self, img_filename):\n",
    "        img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def _convert_to_llava_answer_turn(self, answer):\n",
    "        if isinstance(answer, dict):\n",
    "            return answer\n",
    "        return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n",
    "    def convert_dataframe_row_to_source(self, row):\n",
    "        dict_output = {'question': row['question'], \n",
    "                       # Chosen and rejected based on the sum of the first three quality score\n",
    "                    'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "                    'rejected': self._convert_to_llava_answer_turn(row['rejected']),\n",
    "                    'image': self._get_image(row['image'])}\n",
    "        return dict_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset(inference_data_path,\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]\n",
    "\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')\n",
    "\n",
    "df = pd.read_csv(inference_data_path)\n",
    "\n",
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "# Add each list as a column to the dataframe with its variable name\n",
    "df['ref_win_logp'] = win_logp_list\n",
    "df['ref_rej_logp'] = rej_logp_list\n",
    "df['ref_win_avg_logp'] = win_avg_logp_list\n",
    "df['ref_rej_avg_logp'] = rej_avg_logp_list\n",
    "df['ref_win_per_token_logp'] = win_per_token_logp_list\n",
    "df['ref_rej_per_token_logp'] = rej_per_token_logp_list\n",
    "columns_toconvert = ['question', 'chosen', 'rejected']\n",
    "for col in columns_toconvert:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n",
    "# df.to_json(output_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "columns_toconvert = ['question', 'chosen', 'rejected']\n",
    "for col in columns_toconvert:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': 'gpt',\n",
       " 'value': 'In the image, several baseball players are positioned on the field during a game. A batter is holding a baseball bat and standing next to the home plate, preparing to hit the ball. Behind the batter, a catcher and an umpire are in their respective positions, with the catcher wearing a baseball glove and the umpire closely observing the game. Other players are also present on the field, with one of them holding a baseball bat, indicating that he might be waiting for his turn to bat.\\n\\nThe arrangement of these players and their equipment creates a dynamic scene that captures the essence of a baseball game. The batter, catcher, and umpire are the main focal points, while the other players contribute to the overall atmosphere of the game. The presence of multiple baseball bats suggests that the game is in progress, and the players are actively engaged in the match.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['chosen'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('/home/ubuntu/LLaVA/playground/data/dpo/dpo_logp_rlhf1401.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('/home/ubuntu/LLaVA/playground/data/dpo/dpo_logp_rlhf1401.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'from': 'human', 'value': '<image>\\\\nHow do the elements in the image relate to each other in terms of positioning or composition?'}\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rlhf_v_hall_images/0.jpg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparison = ['ref_rej_logp', 'muffin_ref_rej_logp']\n",
    "df[comparison].plot.hist(bins=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparison = ['ref_win_logp', 'muffin_ref_win_logp']\n",
    "df[comparison].plot.hist(bins=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp_self_sampledRLHFV.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a7572655f6e6f6e2d73706f74227d/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp_self_sampledRLHFV.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "df = pd.read_csv('/home/ubuntu/LLaVA/playground/data/dpo/dpo_rlhf_imagepath.csv')\n",
    "columns_toconvert = ['question', 'chosen', 'rejected']\n",
    "for col in columns_toconvert:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp_self_sampledRLHFV.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a7572655f6e6f6e2d73706f74227d/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp_self_sampledRLHFV.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "df['question'][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1971771691.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df['question'] = df['question'].astype(lambda: x: json.loads(x))\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df['question'] = df['question'].astype(lambda: x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is not of type PIL.Image.Image\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Assuming df['image'].iloc[0] is your image\n",
    "image = df['image'].iloc[0]\n",
    "\n",
    "if isinstance(image, Image.Image):\n",
    "    print(\"The image is of type PIL.Image.Image\")\n",
    "else:\n",
    "    print(\"The image is not of type PIL.Image.Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(df['image'].iloc[0], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp_self_sampled.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a7572655f6e6f6e2d73706f74227d/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp_self_sampled.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39mref_rej_logp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mref_win_logp\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mboxplot()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a7572655f6e6f6e2d73706f74227d/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp_self_sampled.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mllava 1.5 13b PPO based on gpt4v response log p for self-sampled responses\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a7572655f6e6f6e2d73706f74227d/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp_self_sampled.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mylim(\u001b[39m-\u001b[39m\u001b[39m500\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[['ref_rej_logp', 'ref_win_logp']].boxplot()\n",
    "plt.title('llava 1.5 13b PPO based on gpt4v response log p for self-sampled responses')\n",
    "plt.ylim(-500, 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_llava_answer_turn(answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reload['chosen'] = df_reload['chosen'].apply(_convert_to_llava_answer_turn)\n",
    "df_reload['rejected'] = df_reload['rejected'].apply(_convert_to_llava_answer_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       {'from': 'gpt', 'value': 'The dessert in the i...\n",
       "1       {'from': 'gpt', 'value': 'The player's perform...\n",
       "2       {'from': 'gpt', 'value': 'The image displays a...\n",
       "3       {'from': 'gpt', 'value': 'The scenario depicts...\n",
       "4       {'from': 'gpt', 'value': 'The presence of a la...\n",
       "                              ...                        \n",
       "1832    {'from': 'gpt', 'value': 'One possible reason ...\n",
       "1833    {'from': 'gpt', 'value': 'The cyclist is takin...\n",
       "1834    {'from': 'gpt', 'value': 'Several aspects of t...\n",
       "1835    {'from': 'gpt', 'value': 'The image captures a...\n",
       "1836    {'from': 'gpt', 'value': 'The image presents a...\n",
       "Name: rejected, Length: 1837, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reload['rejected']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old draft below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Loading vision tower and image processor...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n",
    "\n",
    "from PIL import Image\n",
    "import math\n",
    "disable_torch_init()\n",
    "model_path = '/home/ubuntu/LLaVA/checkpoints/llava-v1.5-13b'\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_im_start_end= False # Default setting from llava 1.5 \n",
    "\n",
    "IMAGE_FOLDER='/home/ubuntu/train2017'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect previous setup\n",
    "\n",
    "df_muffin_tsv = '/home/ubuntu/muffin/data/RLHF-V-Hall_v0/RLHF-V-Hall_v0-1837.tsv'\n",
    "df = pd.read_csv(df_muffin_tsv, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "          21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "            322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "          29889,  3148,  1001, 29901,   529,  3027, 29958,    13, 11008,   338,\n",
       "            445,  6808,   814,  5929, 12818,   304,  4856,   411,   263, 14225,\n",
       "            304,   720, 29973,   319,  1799,  9047, 13566, 29901,   450,  6808,\n",
       "            814,   297,   278,  1967, 29892,   607,  5680,   263,  8424,   310,\n",
       "            274,  1296, 10664,   297,   377, 16242,   907,   314,   322,   521,\n",
       "            542, 23167,   528,   485,   886, 29892,   723,   367, 10712,  5929,\n",
       "          12818,   304,  4856,   411,   263, 14225,   304,   720, 29889,   910,\n",
       "            338,  1363,   278,   274,  1296,  3528,  5692,   304,   367,   263,\n",
       "           8261,   322, 29156,  5362,  7539, 29892,  1754,  1584,   901,   875,\n",
       "          18499,   491,   278,  6124,   310,   278,   377, 16242,   907,   314,\n",
       "            322,   521,   542, 23167,   528,   485,   886, 29889,   450, 10296,\n",
       "            310, 21054,   943,   322,  1426,  1973, 29892,  1316,   408,   278,\n",
       "          14225,   322,   907, 25934,   377, 16242,   907,   314,   322,   278,\n",
       "          12814,   292,  2586,   725,   404,   310,   278,   521,   542, 23167,\n",
       "          29892,   723,  5517,   367,  1407, 24064,   363,  4856,  1058, 11418,\n",
       "            952, 29156,  3460,   297, 14225,  2578,  1446, 29889,   450, 24329,\n",
       "            310,   278,  6808,   814,   373,   278,  4796, 15284,   411,   263,\n",
       "          13283, 27350,   884, 12778,   304,   278, 12463, 25530, 29892,  3907,\n",
       "            372,   385,  3805,   690,   391,  1821,  2984,   363,  4856,   411,\n",
       "            263, 14225,   304,   720, 29889,     2]),\n",
       "  'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   450,  6808,\n",
       "            814,   297,   278,  1967, 29892,   607,  5680,   263,  8424,   310,\n",
       "            274,  1296, 10664,   297,   377, 16242,   907,   314,   322,   521,\n",
       "            542, 23167,   528,   485,   886, 29892,   723,   367, 10712,  5929,\n",
       "          12818,   304,  4856,   411,   263, 14225,   304,   720, 29889,   910,\n",
       "            338,  1363,   278,   274,  1296,  3528,  5692,   304,   367,   263,\n",
       "           8261,   322, 29156,  5362,  7539, 29892,  1754,  1584,   901,   875,\n",
       "          18499,   491,   278,  6124,   310,   278,   377, 16242,   907,   314,\n",
       "            322,   521,   542, 23167,   528,   485,   886, 29889,   450, 10296,\n",
       "            310, 21054,   943,   322,  1426,  1973, 29892,  1316,   408,   278,\n",
       "          14225,   322,   907, 25934,   377, 16242,   907,   314,   322,   278,\n",
       "          12814,   292,  2586,   725,   404,   310,   278,   521,   542, 23167,\n",
       "          29892,   723,  5517,   367,  1407, 24064,   363,  4856,  1058, 11418,\n",
       "            952, 29156,  3460,   297, 14225,  2578,  1446, 29889,   450, 24329,\n",
       "            310,   278,  6808,   814,   373,   278,  4796, 15284,   411,   263,\n",
       "          13283, 27350,   884, 12778,   304,   278, 12463, 25530, 29892,  3907,\n",
       "            372,   385,  3805,   690,   391,  1821,  2984,   363,  4856,   411,\n",
       "            263, 14225,   304,   720, 29889,     2]),\n",
       "  'image': tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           ...,\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],\n",
       "  \n",
       "          [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           ...,\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
       "  \n",
       "          [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           ...,\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])},\n",
       " {'input_ids': tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "          21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "            322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "          29889,  3148,  1001, 29901,   529,  3027, 29958,    13, 11008,   338,\n",
       "            445,  6808,   814,  5929, 12818,   304,  4856,   411,   263, 14225,\n",
       "            304,   720, 29973,   319,  1799,  9047, 13566, 29901,   910,  6808,\n",
       "            814, 29915, 29879, 25530,   380,  1567,   515,   967,  2473, 29899,\n",
       "          13148,   287,  3829, 29892,   411,  1269,  7546,  3063,  2730,   391,\n",
       "            322,  1602,   328,   296, 29892,  2504,  5921,   385, 17818,   344,\n",
       "            521,   542, 23167, 21054,   272,   393, 29915, 29879,   263, 22037,\n",
       "          25448,  4249,  1906,   411,   263, 14225,   304,   720, 29889,   450,\n",
       "           3144,  2209, 29891,   521,   542, 23167,  9581,  1829,   373,  2246,\n",
       "          29892,   411,   967, 10597, 18459,   322,  8261, 21779, 29892, 12778,\n",
       "            385,  4805,  7546,   310, 29156, 10238, 29889, 12808, 29892,   278,\n",
       "           7689,   682,  1847,   310,  4796,   521,   542, 23167, 17422, 10794,\n",
       "           8128,   451,   871,   263,  1998,  1474,  5929, 12818, 12814,   541,\n",
       "            884,   385,  5684, 14225,   322,   907, 25934, 21054,   272,  1543,\n",
       "            393, 11000,  1532,   411,   278,  6501,   521,   542, 23167, 29889,\n",
       "            450, 12463,   263,   342, 29882,  7492,   310, 28539,   368,  5096,\n",
       "            287, 15359,   322,   278,  5941, 24329,   373,   263,  2560,  4796,\n",
       "          15284,   411,   263, 27350,  7960,   304,   270,   573,   297,  3732,\n",
       "            278,  6808,   814,   451,   871, 13394, 13405,   292,   541,   884,\n",
       "          17739, 29899, 12554, 29891, 29892,   607,   508,   367, 10734,  5929,\n",
       "          12818,   297,  9826, 29915, 29879,  5264,  5745, 29899, 24477,   854,\n",
       "           9257, 29889,     2]),\n",
       "  'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   910,  6808,\n",
       "            814, 29915, 29879, 25530,   380,  1567,   515,   967,  2473, 29899,\n",
       "          13148,   287,  3829, 29892,   411,  1269,  7546,  3063,  2730,   391,\n",
       "            322,  1602,   328,   296, 29892,  2504,  5921,   385, 17818,   344,\n",
       "            521,   542, 23167, 21054,   272,   393, 29915, 29879,   263, 22037,\n",
       "          25448,  4249,  1906,   411,   263, 14225,   304,   720, 29889,   450,\n",
       "           3144,  2209, 29891,   521,   542, 23167,  9581,  1829,   373,  2246,\n",
       "          29892,   411,   967, 10597, 18459,   322,  8261, 21779, 29892, 12778,\n",
       "            385,  4805,  7546,   310, 29156, 10238, 29889, 12808, 29892,   278,\n",
       "           7689,   682,  1847,   310,  4796,   521,   542, 23167, 17422, 10794,\n",
       "           8128,   451,   871,   263,  1998,  1474,  5929, 12818, 12814,   541,\n",
       "            884,   385,  5684, 14225,   322,   907, 25934, 21054,   272,  1543,\n",
       "            393, 11000,  1532,   411,   278,  6501,   521,   542, 23167, 29889,\n",
       "            450, 12463,   263,   342, 29882,  7492,   310, 28539,   368,  5096,\n",
       "            287, 15359,   322,   278,  5941, 24329,   373,   263,  2560,  4796,\n",
       "          15284,   411,   263, 27350,  7960,   304,   270,   573,   297,  3732,\n",
       "            278,  6808,   814,   451,   871, 13394, 13405,   292,   541,   884,\n",
       "          17739, 29899, 12554, 29891, 29892,   607,   508,   367, 10734,  5929,\n",
       "          12818,   297,  9826, 29915, 29879,  5264,  5745, 29899, 24477,   854,\n",
       "           9257, 29889,     2]),\n",
       "  'image': tensor([[[-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           ...,\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113],\n",
       "           [-0.0113, -0.0113, -0.0113,  ..., -0.0113, -0.0113, -0.0113]],\n",
       "  \n",
       "          [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           ...,\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
       "           [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
       "  \n",
       "          [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           ...,\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
       "           [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]])})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llava.train.train import encode_multimodal_preference_sample\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class PreferenceInferenceDataset(torch_data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path,\n",
    "                 tokenizer,\n",
    "                 img_processor,\n",
    "                 use_im_start_end):\n",
    "        self.data = pd.read_csv(data_path, sep='\\t')\n",
    "\n",
    "        self.mm_cfg = {\n",
    "            'image_processor': img_processor,\n",
    "            'is_multimodal': True,\n",
    "            # 'image_token_len': image_token_len, # TODO check if needed\n",
    "            'use_im_start_end': use_im_start_end,\n",
    "            'image_aspect_ratio': 'pad'\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # After encode multimodal preference sample, \n",
    "        # the image would ahve the pixel values values\n",
    "        sample = self.convert_dataframe_row_to_source(self.data.iloc[index])\n",
    "        rej_data_dict, win_data_dict = encode_multimodal_preference_sample(sample, self.tokenizer, self.mm_cfg)\n",
    "        return rej_data_dict, win_data_dict\n",
    "    \n",
    "    def _get_image(self, img_filename):\n",
    "        img_path = os.path.join(IMAGE_FOLDER, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def _convert_to_llava_answer_turn(self, answer):\n",
    "        return {\"from\": \"gpt\", \"value\": answer}\n",
    "\n",
    "    def convert_dataframe_row_to_source(self, row):\n",
    "        dict_output = {'question': row['question'], \n",
    "                       # Chosen and rejected based on the sum of the first three quality score\n",
    "                    'chosen': self._convert_to_llava_answer_turn(row['chosen']),\n",
    "                    'rejected': self._convert_to_llava_answer_turn(row['rejected']),\n",
    "                    'image': self._get_image(row['image'])}\n",
    "        return dict_output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "preference_torch_dataset = PreferenceInferenceDataset('/home/ubuntu/muffin/data/RLHF-V-Hall_v0/RLHF-V-Hall_v0-1837.tsv',\n",
    "                           tokenizer=tokenizer,\n",
    "                           img_processor=image_processor,\n",
    "                           use_im_start_end=False)\n",
    "preference_torch_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pad'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_torch_dataset.mm_cfg['image_aspect_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 336, 336])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _convert_to_llava_answer_turn(self, answer):\n",
    "    return {\"from\": \"gpt\", \"value\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([226])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_torch_dataset[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import preference_collator_fn\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from llava.train.llava_trainer import get_batch_logps\n",
    "dataset = preference_torch_dataset\n",
    "collate_fn = partial(preference_collator_fn, pad_token_id=tokenizer.pad_token_id)\n",
    "dataloader = torch_data.DataLoader(dataset, batch_size=1, collate_fn=collate_fn,\n",
    "                                    num_workers=5, shuffle=False)\n",
    "win_logp_list = []\n",
    "rej_logp_list = []\n",
    "\n",
    "win_avg_logp_list = []\n",
    "rej_avg_logp_list = []\n",
    "\n",
    "win_per_token_logp_list = []\n",
    "rej_per_token_logp_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key in ['win', 'rej']:\n",
    "            input_ids = batch[f'{key}_input_ids'].cuda()\n",
    "            labels = batch[f'{key}_labels'].cuda()\n",
    "            attention_mask = batch[f'{key}_attention_mask'].cuda()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "                images=batch['images'].half().cuda()\n",
    "            )\n",
    "            per_token_logp, log_prob, average_log_prob = get_batch_logps(output.logits, labels, return_all=True)\n",
    "\n",
    "            # print(per_token_logp.shape, input_ids.shape, labels.shape, flush=True)\n",
    "            assert per_token_logp.size(1) >= input_ids.size(1) - 1\n",
    "            per_token_logp = per_token_logp.tolist()\n",
    "            # per_token_logp = [x[:input_ids[i].ne(tokenizer.pad_token_id).sum().item()] for i, x in enumerate(per_token_logp)]\n",
    "            log_prob = log_prob.tolist()\n",
    "            average_log_prob = average_log_prob.tolist()\n",
    "\n",
    "            if key == 'win':\n",
    "                win_logp_list += log_prob\n",
    "                win_avg_logp_list += average_log_prob\n",
    "                win_per_token_logp_list += per_token_logp\n",
    "            else:\n",
    "                rej_logp_list += log_prob\n",
    "                rej_avg_logp_list += average_log_prob\n",
    "                rej_per_token_logp_list += per_token_logp\n",
    "            # print(f'{key} logits in {output.logits.shape}, logp in {log_prob.shape} avg_logp in {average_log_prob.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 92, in __getattr__\n    return self.data[item]\nKeyError: 'shape'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 808, in preference_collator_fn\n    rej_batch = SFT_collator_fn(rej_instances, pad_token_id)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in SFT_collator_fn\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in <genexpr>\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 94, in __getattr__\n    raise AttributeError\nAttributeError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22417a7572655f6e6f6e2d73706f74227d/home/ubuntu/LLaVA/llava/eval/eval_logp_for_ddp.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataloader))\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 92, in __getattr__\n    return self.data[item]\nKeyError: 'shape'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 808, in preference_collator_fn\n    rej_batch = SFT_collator_fn(rej_instances, pad_token_id)\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in SFT_collator_fn\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/LLaVA/llava/train/train.py\", line 789, in <genexpr>\n    if all(x is not None and x.shape == images[0].shape for x in images):\n  File \"/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/transformers/feature_extraction_utils.py\", line 94, in __getattr__\n    raise AttributeError\nAttributeError\n"
     ]
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/_utils.py\u001b[0m(644)\u001b[0;36mreraise\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    642 \u001b[0;31m            \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    643 \u001b[0;31m            \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 644 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    645 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    646 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'images' is not defined\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(1371)\u001b[0;36m_process_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1369 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1370 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1371 \u001b[0;31m            \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1372 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1373 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'images' is not defined\n",
      "*** Invalid frame count (')\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(1345)\u001b[0;36m_next_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1343 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1344 \u001b[0;31m                \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1345 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1346 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1347 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/ubuntu/mambaforge-pypy3/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m(633)\u001b[0;36m__next__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    631 \u001b[0;31m                \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    632 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 633 \u001b[0;31m            \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    634 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    635 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
